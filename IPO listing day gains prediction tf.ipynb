{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1eb002d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c1418b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retail Investors</th>\n",
       "      <th>Qualified Investors</th>\n",
       "      <th>Non Instititional</th>\n",
       "      <th>Total</th>\n",
       "      <th>Listing Day gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.84</td>\n",
       "      <td>150.82</td>\n",
       "      <td>881.96</td>\n",
       "      <td>338.51</td>\n",
       "      <td>148.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.76</td>\n",
       "      <td>7.50</td>\n",
       "      <td>3.46</td>\n",
       "      <td>6.45</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.72</td>\n",
       "      <td>2.88</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.95</td>\n",
       "      <td>-27.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45</td>\n",
       "      <td>27.52</td>\n",
       "      <td>8.15</td>\n",
       "      <td>17.86</td>\n",
       "      <td>22.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.41</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.36</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-5.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>80.49</td>\n",
       "      <td>86.51</td>\n",
       "      <td>172.43</td>\n",
       "      <td>101.91</td>\n",
       "      <td>267.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.92</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.03</td>\n",
       "      <td>-5.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.29</td>\n",
       "      <td>92.17</td>\n",
       "      <td>112.51</td>\n",
       "      <td>82.42</td>\n",
       "      <td>96.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.24</td>\n",
       "      <td>10.36</td>\n",
       "      <td>4.39</td>\n",
       "      <td>5.25</td>\n",
       "      <td>-1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>118.44</td>\n",
       "      <td>175.39</td>\n",
       "      <td>973.99</td>\n",
       "      <td>318.04</td>\n",
       "      <td>181.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.15</td>\n",
       "      <td>26.47</td>\n",
       "      <td>11.37</td>\n",
       "      <td>11.47</td>\n",
       "      <td>9.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.09</td>\n",
       "      <td>13.07</td>\n",
       "      <td>1.32</td>\n",
       "      <td>4.54</td>\n",
       "      <td>16.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.42</td>\n",
       "      <td>86.02</td>\n",
       "      <td>155.44</td>\n",
       "      <td>64.59</td>\n",
       "      <td>53.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.35</td>\n",
       "      <td>32.41</td>\n",
       "      <td>33.91</td>\n",
       "      <td>17.20</td>\n",
       "      <td>-0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.34</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.21</td>\n",
       "      <td>-0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.75</td>\n",
       "      <td>35.45</td>\n",
       "      <td>41.00</td>\n",
       "      <td>20.29</td>\n",
       "      <td>-7.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.74</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.74</td>\n",
       "      <td>-7.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>39.52</td>\n",
       "      <td>95.27</td>\n",
       "      <td>213.06</td>\n",
       "      <td>116.71</td>\n",
       "      <td>36.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24.54</td>\n",
       "      <td>24.10</td>\n",
       "      <td>15.91</td>\n",
       "      <td>22.56</td>\n",
       "      <td>-11.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>42.04</td>\n",
       "      <td>49.83</td>\n",
       "      <td>116.30</td>\n",
       "      <td>64.40</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40.10</td>\n",
       "      <td>17.67</td>\n",
       "      <td>5.36</td>\n",
       "      <td>22.68</td>\n",
       "      <td>10.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>24.49</td>\n",
       "      <td>143.58</td>\n",
       "      <td>360.11</td>\n",
       "      <td>130.43</td>\n",
       "      <td>29.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.00</td>\n",
       "      <td>37.30</td>\n",
       "      <td>125.62</td>\n",
       "      <td>45.07</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>35.68</td>\n",
       "      <td>186.96</td>\n",
       "      <td>516.96</td>\n",
       "      <td>182.04</td>\n",
       "      <td>113.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7.87</td>\n",
       "      <td>54.71</td>\n",
       "      <td>34.80</td>\n",
       "      <td>40.38</td>\n",
       "      <td>64.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12.57</td>\n",
       "      <td>168.58</td>\n",
       "      <td>238.04</td>\n",
       "      <td>102.58</td>\n",
       "      <td>107.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.20</td>\n",
       "      <td>159.93</td>\n",
       "      <td>211.12</td>\n",
       "      <td>95.54</td>\n",
       "      <td>75.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11.30</td>\n",
       "      <td>42.95</td>\n",
       "      <td>51.88</td>\n",
       "      <td>29.04</td>\n",
       "      <td>15.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.34</td>\n",
       "      <td>84.88</td>\n",
       "      <td>73.26</td>\n",
       "      <td>45.62</td>\n",
       "      <td>42.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.90</td>\n",
       "      <td>5.26</td>\n",
       "      <td>1.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>19.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>11.64</td>\n",
       "      <td>153.45</td>\n",
       "      <td>339.98</td>\n",
       "      <td>120.93</td>\n",
       "      <td>22.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.58</td>\n",
       "      <td>3.46</td>\n",
       "      <td>0.39</td>\n",
       "      <td>2.28</td>\n",
       "      <td>24.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.09</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.37</td>\n",
       "      <td>-10.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.40</td>\n",
       "      <td>3.05</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.36</td>\n",
       "      <td>-5.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2.82</td>\n",
       "      <td>2.76</td>\n",
       "      <td>1.91</td>\n",
       "      <td>2.61</td>\n",
       "      <td>-14.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>13.13</td>\n",
       "      <td>5.11</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.98</td>\n",
       "      <td>17.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>75.29</td>\n",
       "      <td>103.77</td>\n",
       "      <td>389.89</td>\n",
       "      <td>175.46</td>\n",
       "      <td>44.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3.44</td>\n",
       "      <td>5.21</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.82</td>\n",
       "      <td>-5.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20.10</td>\n",
       "      <td>175.43</td>\n",
       "      <td>217.62</td>\n",
       "      <td>106.81</td>\n",
       "      <td>26.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10.77</td>\n",
       "      <td>65.74</td>\n",
       "      <td>97.42</td>\n",
       "      <td>44.06</td>\n",
       "      <td>-6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>70.40</td>\n",
       "      <td>77.53</td>\n",
       "      <td>382.21</td>\n",
       "      <td>159.33</td>\n",
       "      <td>10.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>28.40</td>\n",
       "      <td>164.99</td>\n",
       "      <td>650.79</td>\n",
       "      <td>200.79</td>\n",
       "      <td>87.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>11.84</td>\n",
       "      <td>67.45</td>\n",
       "      <td>271.15</td>\n",
       "      <td>83.29</td>\n",
       "      <td>31.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>16.78</td>\n",
       "      <td>65.14</td>\n",
       "      <td>73.25</td>\n",
       "      <td>42.39</td>\n",
       "      <td>28.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>166.65</td>\n",
       "      <td>3.10</td>\n",
       "      <td>31.59</td>\n",
       "      <td>39.93</td>\n",
       "      <td>61.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>26.04</td>\n",
       "      <td>8.02</td>\n",
       "      <td>32.72</td>\n",
       "      <td>18.03</td>\n",
       "      <td>13.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6.59</td>\n",
       "      <td>52.53</td>\n",
       "      <td>39.00</td>\n",
       "      <td>26.66</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3.66</td>\n",
       "      <td>3.78</td>\n",
       "      <td>2.67</td>\n",
       "      <td>3.49</td>\n",
       "      <td>-4.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>15.93</td>\n",
       "      <td>189.57</td>\n",
       "      <td>263.05</td>\n",
       "      <td>117.02</td>\n",
       "      <td>110.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>68.15</td>\n",
       "      <td>86.64</td>\n",
       "      <td>354.11</td>\n",
       "      <td>156.65</td>\n",
       "      <td>130.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Retail Investors  Qualified Investors  Non Instititional   Total  \\\n",
       "0             123.84               150.82             881.96  338.51   \n",
       "1               7.76                 7.50               3.46    6.45   \n",
       "2               1.72                 2.88               0.25    1.95   \n",
       "3               3.45                27.52               8.15   17.86   \n",
       "4               1.41                 1.45               2.36    1.63   \n",
       "5              80.49                86.51             172.43  101.91   \n",
       "6               5.92                 1.65               0.21    2.03   \n",
       "7              12.29                92.17             112.51   82.42   \n",
       "8               3.24                10.36               4.39    5.25   \n",
       "9             118.44               175.39             973.99  318.04   \n",
       "10              3.15                26.47              11.37   11.47   \n",
       "11              1.09                13.07               1.32    4.54   \n",
       "12             13.42                86.02             155.44   64.59   \n",
       "13              1.35                32.41              33.91   17.20   \n",
       "14              2.34                 2.75               1.05    2.21   \n",
       "15              2.75                35.45              41.00   20.29   \n",
       "16              0.74                 4.40               0.67    1.74   \n",
       "17             39.52                95.27             213.06  116.71   \n",
       "18             24.54                24.10              15.91   22.56   \n",
       "19             42.04                49.83             116.30   64.40   \n",
       "20             40.10                17.67               5.36   22.68   \n",
       "21             24.49               143.58             360.11  130.43   \n",
       "22             15.00                37.30             125.62   45.07   \n",
       "23             35.68               186.96             516.96  182.04   \n",
       "24              7.87                54.71              34.80   40.38   \n",
       "25             12.57               168.58             238.04  102.58   \n",
       "26              9.20               159.93             211.12   95.54   \n",
       "27             11.30                42.95              51.88   29.04   \n",
       "28             11.34                84.88              73.26   45.62   \n",
       "29              2.90                 5.26               1.89    3.84   \n",
       "30             11.64               153.45             339.98  120.93   \n",
       "31              1.58                 3.46               0.39    2.28   \n",
       "32              3.09                 2.18               1.31    2.37   \n",
       "33              0.40                 3.05               1.44    1.36   \n",
       "34              2.82                 2.76               1.91    2.61   \n",
       "35             13.13                 5.11               3.10    5.98   \n",
       "36             75.29               103.77             389.89  175.46   \n",
       "37              3.44                 5.21               2.84    3.82   \n",
       "38             20.10               175.43             217.62  106.81   \n",
       "39             10.77                65.74              97.42   44.06   \n",
       "40             70.40                77.53             382.21  159.33   \n",
       "41             28.40               164.99             650.79  200.79   \n",
       "42             11.84                67.45             271.15   83.29   \n",
       "43             16.78                65.14              73.25   42.39   \n",
       "44            166.65                 3.10              31.59   39.93   \n",
       "45             26.04                 8.02              32.72   18.03   \n",
       "46              6.59                52.53              39.00   26.66   \n",
       "47              3.66                 3.78               2.67    3.49   \n",
       "48             15.93               189.57             263.05  117.02   \n",
       "49             68.15                86.64             354.11  156.65   \n",
       "\n",
       "    Listing Day gain  \n",
       "0             148.10  \n",
       "1               4.04  \n",
       "2             -27.44  \n",
       "3              22.68  \n",
       "4              -5.87  \n",
       "5             267.18  \n",
       "6              -5.74  \n",
       "7              96.27  \n",
       "8              -1.97  \n",
       "9             181.40  \n",
       "10              9.95  \n",
       "11             16.20  \n",
       "12             53.20  \n",
       "13             -0.28  \n",
       "14             -0.92  \n",
       "15             -7.85  \n",
       "16             -7.19  \n",
       "17             36.22  \n",
       "18            -11.74  \n",
       "19              2.52  \n",
       "20             10.25  \n",
       "21             29.67  \n",
       "22              3.61  \n",
       "23            113.50  \n",
       "24             64.87  \n",
       "25            107.64  \n",
       "26             75.89  \n",
       "27             15.64  \n",
       "28             42.29  \n",
       "29             19.70  \n",
       "30             22.71  \n",
       "31             24.12  \n",
       "32            -10.80  \n",
       "33             -5.77  \n",
       "34            -14.54  \n",
       "35             17.56  \n",
       "36             44.60  \n",
       "37             -5.03  \n",
       "38             26.19  \n",
       "39             -6.58  \n",
       "40             10.43  \n",
       "41             87.62  \n",
       "42             31.74  \n",
       "43             28.19  \n",
       "44             61.44  \n",
       "45             13.88  \n",
       "46              0.87  \n",
       "47             -4.62  \n",
       "48            110.00  \n",
       "49            130.70  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_csv('IPO.csv')\n",
    "df_drop = ['Employee','Company Name']\n",
    "df=df_1.drop(df_drop,axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f5148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "X= df.drop('Listing Day gain',axis=1)\n",
    "y=df['Listing Day gain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b667a06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retail Investors</th>\n",
       "      <th>Qualified Investors</th>\n",
       "      <th>Non Instititional</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.84</td>\n",
       "      <td>150.82</td>\n",
       "      <td>881.96</td>\n",
       "      <td>338.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.76</td>\n",
       "      <td>7.50</td>\n",
       "      <td>3.46</td>\n",
       "      <td>6.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.72</td>\n",
       "      <td>2.88</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45</td>\n",
       "      <td>27.52</td>\n",
       "      <td>8.15</td>\n",
       "      <td>17.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.41</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.36</td>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>80.49</td>\n",
       "      <td>86.51</td>\n",
       "      <td>172.43</td>\n",
       "      <td>101.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.92</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.29</td>\n",
       "      <td>92.17</td>\n",
       "      <td>112.51</td>\n",
       "      <td>82.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.24</td>\n",
       "      <td>10.36</td>\n",
       "      <td>4.39</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>118.44</td>\n",
       "      <td>175.39</td>\n",
       "      <td>973.99</td>\n",
       "      <td>318.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.15</td>\n",
       "      <td>26.47</td>\n",
       "      <td>11.37</td>\n",
       "      <td>11.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.09</td>\n",
       "      <td>13.07</td>\n",
       "      <td>1.32</td>\n",
       "      <td>4.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.42</td>\n",
       "      <td>86.02</td>\n",
       "      <td>155.44</td>\n",
       "      <td>64.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.35</td>\n",
       "      <td>32.41</td>\n",
       "      <td>33.91</td>\n",
       "      <td>17.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.34</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.75</td>\n",
       "      <td>35.45</td>\n",
       "      <td>41.00</td>\n",
       "      <td>20.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.74</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>39.52</td>\n",
       "      <td>95.27</td>\n",
       "      <td>213.06</td>\n",
       "      <td>116.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24.54</td>\n",
       "      <td>24.10</td>\n",
       "      <td>15.91</td>\n",
       "      <td>22.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>42.04</td>\n",
       "      <td>49.83</td>\n",
       "      <td>116.30</td>\n",
       "      <td>64.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40.10</td>\n",
       "      <td>17.67</td>\n",
       "      <td>5.36</td>\n",
       "      <td>22.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>24.49</td>\n",
       "      <td>143.58</td>\n",
       "      <td>360.11</td>\n",
       "      <td>130.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.00</td>\n",
       "      <td>37.30</td>\n",
       "      <td>125.62</td>\n",
       "      <td>45.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>35.68</td>\n",
       "      <td>186.96</td>\n",
       "      <td>516.96</td>\n",
       "      <td>182.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7.87</td>\n",
       "      <td>54.71</td>\n",
       "      <td>34.80</td>\n",
       "      <td>40.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12.57</td>\n",
       "      <td>168.58</td>\n",
       "      <td>238.04</td>\n",
       "      <td>102.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.20</td>\n",
       "      <td>159.93</td>\n",
       "      <td>211.12</td>\n",
       "      <td>95.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11.30</td>\n",
       "      <td>42.95</td>\n",
       "      <td>51.88</td>\n",
       "      <td>29.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.34</td>\n",
       "      <td>84.88</td>\n",
       "      <td>73.26</td>\n",
       "      <td>45.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.90</td>\n",
       "      <td>5.26</td>\n",
       "      <td>1.89</td>\n",
       "      <td>3.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>11.64</td>\n",
       "      <td>153.45</td>\n",
       "      <td>339.98</td>\n",
       "      <td>120.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.58</td>\n",
       "      <td>3.46</td>\n",
       "      <td>0.39</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.09</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.40</td>\n",
       "      <td>3.05</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2.82</td>\n",
       "      <td>2.76</td>\n",
       "      <td>1.91</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>13.13</td>\n",
       "      <td>5.11</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>75.29</td>\n",
       "      <td>103.77</td>\n",
       "      <td>389.89</td>\n",
       "      <td>175.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3.44</td>\n",
       "      <td>5.21</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20.10</td>\n",
       "      <td>175.43</td>\n",
       "      <td>217.62</td>\n",
       "      <td>106.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10.77</td>\n",
       "      <td>65.74</td>\n",
       "      <td>97.42</td>\n",
       "      <td>44.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>70.40</td>\n",
       "      <td>77.53</td>\n",
       "      <td>382.21</td>\n",
       "      <td>159.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>28.40</td>\n",
       "      <td>164.99</td>\n",
       "      <td>650.79</td>\n",
       "      <td>200.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>11.84</td>\n",
       "      <td>67.45</td>\n",
       "      <td>271.15</td>\n",
       "      <td>83.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>16.78</td>\n",
       "      <td>65.14</td>\n",
       "      <td>73.25</td>\n",
       "      <td>42.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>166.65</td>\n",
       "      <td>3.10</td>\n",
       "      <td>31.59</td>\n",
       "      <td>39.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>26.04</td>\n",
       "      <td>8.02</td>\n",
       "      <td>32.72</td>\n",
       "      <td>18.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6.59</td>\n",
       "      <td>52.53</td>\n",
       "      <td>39.00</td>\n",
       "      <td>26.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3.66</td>\n",
       "      <td>3.78</td>\n",
       "      <td>2.67</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>15.93</td>\n",
       "      <td>189.57</td>\n",
       "      <td>263.05</td>\n",
       "      <td>117.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>68.15</td>\n",
       "      <td>86.64</td>\n",
       "      <td>354.11</td>\n",
       "      <td>156.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Retail Investors  Qualified Investors  Non Instititional   Total\n",
       "0             123.84               150.82             881.96  338.51\n",
       "1               7.76                 7.50               3.46    6.45\n",
       "2               1.72                 2.88               0.25    1.95\n",
       "3               3.45                27.52               8.15   17.86\n",
       "4               1.41                 1.45               2.36    1.63\n",
       "5              80.49                86.51             172.43  101.91\n",
       "6               5.92                 1.65               0.21    2.03\n",
       "7              12.29                92.17             112.51   82.42\n",
       "8               3.24                10.36               4.39    5.25\n",
       "9             118.44               175.39             973.99  318.04\n",
       "10              3.15                26.47              11.37   11.47\n",
       "11              1.09                13.07               1.32    4.54\n",
       "12             13.42                86.02             155.44   64.59\n",
       "13              1.35                32.41              33.91   17.20\n",
       "14              2.34                 2.75               1.05    2.21\n",
       "15              2.75                35.45              41.00   20.29\n",
       "16              0.74                 4.40               0.67    1.74\n",
       "17             39.52                95.27             213.06  116.71\n",
       "18             24.54                24.10              15.91   22.56\n",
       "19             42.04                49.83             116.30   64.40\n",
       "20             40.10                17.67               5.36   22.68\n",
       "21             24.49               143.58             360.11  130.43\n",
       "22             15.00                37.30             125.62   45.07\n",
       "23             35.68               186.96             516.96  182.04\n",
       "24              7.87                54.71              34.80   40.38\n",
       "25             12.57               168.58             238.04  102.58\n",
       "26              9.20               159.93             211.12   95.54\n",
       "27             11.30                42.95              51.88   29.04\n",
       "28             11.34                84.88              73.26   45.62\n",
       "29              2.90                 5.26               1.89    3.84\n",
       "30             11.64               153.45             339.98  120.93\n",
       "31              1.58                 3.46               0.39    2.28\n",
       "32              3.09                 2.18               1.31    2.37\n",
       "33              0.40                 3.05               1.44    1.36\n",
       "34              2.82                 2.76               1.91    2.61\n",
       "35             13.13                 5.11               3.10    5.98\n",
       "36             75.29               103.77             389.89  175.46\n",
       "37              3.44                 5.21               2.84    3.82\n",
       "38             20.10               175.43             217.62  106.81\n",
       "39             10.77                65.74              97.42   44.06\n",
       "40             70.40                77.53             382.21  159.33\n",
       "41             28.40               164.99             650.79  200.79\n",
       "42             11.84                67.45             271.15   83.29\n",
       "43             16.78                65.14              73.25   42.39\n",
       "44            166.65                 3.10              31.59   39.93\n",
       "45             26.04                 8.02              32.72   18.03\n",
       "46              6.59                52.53              39.00   26.66\n",
       "47              3.66                 3.78               2.67    3.49\n",
       "48             15.93               189.57             263.05  117.02\n",
       "49             68.15                86.64             354.11  156.65"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c18b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e17cf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 4), (40,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb1a12b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     148.10\n",
       "1       4.04\n",
       "2     -27.44\n",
       "3      22.68\n",
       "4      -5.87\n",
       "5     267.18\n",
       "6      -5.74\n",
       "7      96.27\n",
       "8      -1.97\n",
       "9     181.40\n",
       "10      9.95\n",
       "11     16.20\n",
       "12     53.20\n",
       "13     -0.28\n",
       "14     -0.92\n",
       "15     -7.85\n",
       "16     -7.19\n",
       "17     36.22\n",
       "18    -11.74\n",
       "19      2.52\n",
       "20     10.25\n",
       "21     29.67\n",
       "22      3.61\n",
       "23    113.50\n",
       "24     64.87\n",
       "25    107.64\n",
       "26     75.89\n",
       "27     15.64\n",
       "28     42.29\n",
       "29     19.70\n",
       "30     22.71\n",
       "31     24.12\n",
       "32    -10.80\n",
       "33     -5.77\n",
       "34    -14.54\n",
       "35     17.56\n",
       "36     44.60\n",
       "37     -5.03\n",
       "38     26.19\n",
       "39     -6.58\n",
       "40     10.43\n",
       "41     87.62\n",
       "42     31.74\n",
       "43     28.19\n",
       "44     61.44\n",
       "45     13.88\n",
       "46      0.87\n",
       "47     -4.62\n",
       "48    110.00\n",
       "49    130.70\n",
       "Name: Listing Day gain, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "65e75062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 1s 5ms/step - loss: 65.2300 - mae: 65.2300\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 52.3135 - mae: 52.3135\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 41.4025 - mae: 41.4025\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 34.2729 - mae: 34.2729\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 28.2384 - mae: 28.2384\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 27.3502 - mae: 27.3502\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 31.8215 - mae: 31.8215\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 32.9930 - mae: 32.9930\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 31.0365 - mae: 31.0365\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 27.2482 - mae: 27.2482\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 25.9988 - mae: 25.9988\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 26.1882 - mae: 26.1882\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 27.6784 - mae: 27.6784\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 27.4881 - mae: 27.4881\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 26.6583 - mae: 26.6583\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 25.3158 - mae: 25.3158\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 23.3850 - mae: 23.3850\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.8637 - mae: 22.8637\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 23.3329 - mae: 23.3329\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 24.1408 - mae: 24.1408\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 24.4396 - mae: 24.4396\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 24.1596 - mae: 24.1596\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 23.5323 - mae: 23.5323\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.5027 - mae: 22.5027\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.6524 - mae: 22.6524\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 23.9631 - mae: 23.9631\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 24.2168 - mae: 24.2168\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 23.8566 - mae: 23.8566\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 22.9128 - mae: 22.9128\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 22.4562 - mae: 22.4562\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 22.6182 - mae: 22.6182\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 23.1056 - mae: 23.1056\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 22.9034 - mae: 22.9034\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.3549 - mae: 22.3549\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.6013 - mae: 22.6013\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.8989 - mae: 22.8989\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 22.7012 - mae: 22.7012\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.5419 - mae: 22.5419\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.0360 - mae: 22.0360\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.4751 - mae: 22.4751\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.6952 - mae: 22.6952\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.2759 - mae: 22.2759\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.1521 - mae: 22.1521\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.2860 - mae: 22.2860\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 22.3436 - mae: 22.3436\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.2872 - mae: 22.2872\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.2575 - mae: 22.2575\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 22.0165 - mae: 22.0165\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.1057 - mae: 22.1057\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.1635 - mae: 22.1635\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.8024 - mae: 21.8024\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.0117 - mae: 22.0117\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.8482 - mae: 21.8482\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.7332 - mae: 21.7332\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.9127 - mae: 21.9127\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.7808 - mae: 21.7808\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.7494 - mae: 21.7494\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.9558 - mae: 21.9558\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 22.0201 - mae: 22.0201\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.8606 - mae: 21.8606\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.8020 - mae: 21.8020\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.6899 - mae: 21.6899\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.7813 - mae: 21.7813\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.7111 - mae: 21.7111\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.6195 - mae: 21.6195\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.4726 - mae: 21.4726\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.5570 - mae: 21.5570\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.5078 - mae: 21.5078\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 21.4700 - mae: 21.4700\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.6574 - mae: 21.6574\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.3492 - mae: 21.3492\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 22.0377 - mae: 22.0377\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.9588 - mae: 21.9588\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.6332 - mae: 21.6332\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.4245 - mae: 21.4245\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.5579 - mae: 21.5579\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.4272 - mae: 21.4272\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 21.8045 - mae: 21.8045\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.4682 - mae: 21.4682\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.6004 - mae: 21.6004\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.8714 - mae: 21.8714\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.6888 - mae: 21.6888\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.5481 - mae: 21.5481\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.6221 - mae: 21.6221\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.6685 - mae: 21.6685\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.3402 - mae: 21.3402\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 21.5430 - mae: 21.5430\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 21.9122 - mae: 21.9122\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.8362 - mae: 21.8362\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.5870 - mae: 21.5870\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.2350 - mae: 21.2350\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.4719 - mae: 21.4719\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.6297 - mae: 21.6297\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.2821 - mae: 21.2821\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.2578 - mae: 21.2578\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 21.4480 - mae: 21.4480\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.4702 - mae: 21.4702\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.5344 - mae: 21.5344\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.3075 - mae: 21.3075\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.4715 - mae: 21.4715\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.8210 - mae: 21.8210\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.4352 - mae: 21.4352\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.2547 - mae: 21.2547\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.6663 - mae: 21.6663\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.5240 - mae: 21.5240\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.3582 - mae: 21.3582\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.9559 - mae: 20.9559\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.2938 - mae: 21.2938\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.5521 - mae: 21.5521\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.2080 - mae: 21.2080\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.9559 - mae: 20.9559\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.3039 - mae: 21.3039\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.3735 - mae: 21.3735\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.2877 - mae: 21.2877\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.1062 - mae: 21.1062\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.1813 - mae: 21.1813\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.9365 - mae: 20.9365\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.2710 - mae: 21.2710\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.3225 - mae: 21.3225\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 21.2507 - mae: 21.2507\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.9108 - mae: 20.9108\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.8645 - mae: 20.8645\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.0001 - mae: 21.0001\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.8118 - mae: 20.8118\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.7163 - mae: 20.7163\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.6471 - mae: 20.6471\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.7912 - mae: 20.7912\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 20.6649 - mae: 20.6649\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.9086 - mae: 20.9086\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.7475 - mae: 20.7475\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.7822 - mae: 20.7822\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.8082 - mae: 20.8082\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.5828 - mae: 20.5828\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.7761 - mae: 20.7761\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.7623 - mae: 20.7623\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 20.5382 - mae: 20.5382\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.0119 - mae: 21.0119\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.0298 - mae: 21.0298\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.6052 - mae: 20.6052\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.6498 - mae: 20.6498\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.7443 - mae: 20.7443\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.8124 - mae: 20.8124\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.6513 - mae: 20.6513\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.7476 - mae: 20.7476\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.8015 - mae: 20.8015\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.8007 - mae: 20.8007\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.5551 - mae: 20.5551\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.6823 - mae: 20.6823\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.3484 - mae: 20.3484\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.6420 - mae: 20.6420\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.0316 - mae: 21.0316\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.8814 - mae: 20.8814\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 20.4165 - mae: 20.4165\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.4650 - mae: 20.4650\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.8365 - mae: 20.8365\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.6792 - mae: 20.6792\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.6548 - mae: 20.6548\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.6796 - mae: 20.6796\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.5440 - mae: 20.5440\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.3381 - mae: 20.3381\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.3305 - mae: 20.3305\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.4600 - mae: 20.4600\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.4409 - mae: 20.4409\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 20.5170 - mae: 20.5170\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 20.4057 - mae: 20.4057\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.2927 - mae: 20.2927\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.1738 - mae: 20.1738\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.3149 - mae: 20.3149\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.2482 - mae: 20.2482\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.5332 - mae: 20.5332\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.5757 - mae: 20.5757\n",
      "Epoch 172/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 20.1595 - mae: 20.1595\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.6794 - mae: 20.6794\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.6322 - mae: 20.6322\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.5777 - mae: 20.5777\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.2455 - mae: 20.2455\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.0250 - mae: 21.0250\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.6496 - mae: 20.6496\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.6247 - mae: 20.6247\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.1284 - mae: 21.1284\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.1487 - mae: 21.1487\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.8548 - mae: 20.8548\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.2164 - mae: 20.2164\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 20.3963 - mae: 20.3963\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.2175 - mae: 20.2175\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.1969 - mae: 20.1969\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.1103 - mae: 20.1103\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.2525 - mae: 20.2525\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.2053 - mae: 20.2053\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.2164 - mae: 20.2164\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.4314 - mae: 20.4314\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 20.1987 - mae: 20.1987\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.2597 - mae: 20.2597\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.1475 - mae: 20.1475\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.0776 - mae: 20.0776\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.4986 - mae: 20.4986\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.4125 - mae: 20.4125\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.1588 - mae: 20.1588\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.4992 - mae: 20.4992\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.3625 - mae: 20.3625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1edd8eee520>"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building basic model\n",
    "# Sequential\n",
    "model_1 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(10,activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_1.compile(loss=tf.keras.losses.mae,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=.01),\n",
    "              metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "model_1.fit(X,y,epochs=200,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "7cdcd989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 177ms/step - loss: 15.3967 - mae: 15.3967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15.396677017211914, 15.396677017211914]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "06c96ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_33 (Dense)            (None, 10)                50        \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 5)                 55        \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 111\n",
      "Trainable params: 111\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "1d1cc388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 1s 5ms/step - loss: 25.5222 - mae: 25.5222\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 28.6927 - mae: 28.6927\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 28.2498 - mae: 28.2498\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 28.5506 - mae: 28.5506\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 25.8374 - mae: 25.8374\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 23.7312 - mae: 23.7312\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 24.1531 - mae: 24.1531\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 25.2163 - mae: 25.2163\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 24.2018 - mae: 24.2018\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 23.7937 - mae: 23.7937\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 24.2417 - mae: 24.2417\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 24.8493 - mae: 24.8493\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 24.5685 - mae: 24.5685\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 23.4284 - mae: 23.4284\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 23.6237 - mae: 23.6237\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 26.9038 - mae: 26.9038\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 27.4586 - mae: 27.4586\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 25.1269 - mae: 25.1269\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 23.3426 - mae: 23.3426\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.7663 - mae: 22.7663\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.9555 - mae: 22.9555\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 23.0047 - mae: 23.0047\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 22.5664 - mae: 22.5664\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.7488 - mae: 22.7488\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 22.6963 - mae: 22.6963\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.2021 - mae: 22.2021\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.0769 - mae: 22.0769\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.9004 - mae: 22.9004\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.6501 - mae: 22.6501\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 22.5587 - mae: 22.5587\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.4744 - mae: 22.4744\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.4538 - mae: 22.4538\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.1267 - mae: 22.1267\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 22.1959 - mae: 22.1959\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 22.2018 - mae: 22.2018\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.8693 - mae: 21.8693\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.9905 - mae: 21.9905\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.9176 - mae: 21.9176\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.6589 - mae: 21.6589\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.9996 - mae: 21.9996\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.6852 - mae: 22.6852\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 22.2174 - mae: 22.2174\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.6572 - mae: 21.6572\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.0639 - mae: 22.0639\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.4169 - mae: 22.4169\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.2318 - mae: 22.2318\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 21.8577 - mae: 21.8577\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.7802 - mae: 21.7802\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 22.4315 - mae: 22.4315\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.9477 - mae: 21.9477\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.6170 - mae: 21.6170\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.9396 - mae: 21.9396\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.1570 - mae: 22.1570\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.1543 - mae: 22.1543\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.0819 - mae: 21.0819\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 21.3354 - mae: 21.3354\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.3321 - mae: 21.3321\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.2752 - mae: 21.2752\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.1498 - mae: 22.1498\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 22.5755 - mae: 22.5755\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 22.2728 - mae: 22.2728\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.8413 - mae: 21.8413\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.5106 - mae: 21.5106\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.6778 - mae: 21.6778\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.6534 - mae: 21.6534\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.2001 - mae: 21.2001\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.0565 - mae: 21.0565\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.9983 - mae: 21.9983\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.7619 - mae: 22.7619\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.5576 - mae: 22.5576\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.1503 - mae: 22.1503\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.2502 - mae: 21.2502\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.3272 - mae: 21.3272\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.0370 - mae: 21.0370\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.2701 - mae: 21.2701\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.2739 - mae: 21.2739\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.3176 - mae: 21.3176\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 21.3755 - mae: 21.3755\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.9176 - mae: 20.9176\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.0104 - mae: 21.0104\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.4739 - mae: 20.4739\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.1505 - mae: 21.1505\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.7757 - mae: 21.7757\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.8875 - mae: 22.8875\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 21.6698 - mae: 21.6698\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.4604 - mae: 20.4604\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 5ms/step - loss: 20.4596 - mae: 20.4596\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.7472 - mae: 20.7472\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.1289 - mae: 21.1289\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.6267 - mae: 20.6267\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.3309 - mae: 20.3309\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.0089 - mae: 21.0089\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 22.2280 - mae: 22.2280\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.2230 - mae: 21.2230\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.0143 - mae: 20.0143\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.5786 - mae: 20.5786\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.7280 - mae: 20.7280\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.4838 - mae: 20.4838\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.1988 - mae: 20.1988\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 20.3727 - mae: 20.3727\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.3234 - mae: 20.3234\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.2330 - mae: 20.2330\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.3239 - mae: 20.3239\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.8931 - mae: 19.8931\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 19.5582 - mae: 19.5582\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.0281 - mae: 20.0281\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.1083 - mae: 21.1083\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.0486 - mae: 20.0486\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.2190 - mae: 20.2190\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.5646 - mae: 20.5646\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.2187 - mae: 20.2187\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.6307 - mae: 20.6307\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 21.2915 - mae: 21.2915\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.1733 - mae: 20.1733\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.0447 - mae: 20.0447\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 20.9064 - mae: 20.9064\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 21.5278 - mae: 21.5278\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.6523 - mae: 20.6523\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 18.8965 - mae: 18.8965\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.0073 - mae: 22.0073\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 22.2463 - mae: 22.2463\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.7651 - mae: 19.7651\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.2477 - mae: 21.2477\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 0s/step - loss: 21.7873 - mae: 21.7873\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.7376 - mae: 21.7376\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 21.5000 - mae: 21.5000\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 20.8372 - mae: 20.8372\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 20.3173 - mae: 20.3173\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.7667 - mae: 19.7667\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 19.3685 - mae: 19.3685\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 19.3649 - mae: 19.3649\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 18.8375 - mae: 18.8375\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 19.6245 - mae: 19.6245\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 19.8731 - mae: 19.8731\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.4865 - mae: 19.4865\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.6851 - mae: 18.6851\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.8815 - mae: 19.8815\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 19.7869 - mae: 19.7869\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.8487 - mae: 18.8487\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 19.1875 - mae: 19.1875\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.9821 - mae: 19.9821\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 19.2229 - mae: 19.2229\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.7625 - mae: 19.7625\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.9662 - mae: 19.9662\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.5833 - mae: 19.5833\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 19.0722 - mae: 19.0722\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.7009 - mae: 18.7009\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.7415 - mae: 19.7415\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.1103 - mae: 19.1103\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.1799 - mae: 19.1799\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.0476 - mae: 19.0476\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.5839 - mae: 18.5839\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18.5949 - mae: 18.5949\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.4507 - mae: 18.4507\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 18.3300 - mae: 18.3300\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 18.3894 - mae: 18.3894\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 18.3035 - mae: 18.3035\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 18.5007 - mae: 18.5007\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.1326 - mae: 18.1326\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 18.3021 - mae: 18.3021\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 18.0796 - mae: 18.0796\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 18.4265 - mae: 18.4265\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 18.1912 - mae: 18.1912\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 18.1065 - mae: 18.1065\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 18.6397 - mae: 18.6397\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 18.8216 - mae: 18.8216\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 18.9763 - mae: 18.9763\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.6623 - mae: 18.6623\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.3950 - mae: 18.3950\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.9502 - mae: 17.9502\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 17.4420 - mae: 17.4420\n",
      "Epoch 172/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 5ms/step - loss: 17.7364 - mae: 17.7364\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 17.3015 - mae: 17.3015\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.5720 - mae: 18.5720\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 17.8805 - mae: 17.8805\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.6176 - mae: 16.6176\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 17.5054 - mae: 17.5054\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 16.2354 - mae: 16.2354\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 17.7449 - mae: 17.7449\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.1577 - mae: 19.1577\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 19.0329 - mae: 19.0329\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 18.3274 - mae: 18.3274\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.2046 - mae: 17.2046\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 17.3116 - mae: 17.3116\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 18.4133 - mae: 18.4133\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.3616 - mae: 17.3616\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 16.4702 - mae: 16.4702\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 16.8409 - mae: 16.8409\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.6391 - mae: 15.6391\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.1215 - mae: 17.1215\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.7710 - mae: 15.7710\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 16.6195 - mae: 16.6195\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 17.5421 - mae: 17.5421\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 16.9333 - mae: 16.9333\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.8321 - mae: 14.8321\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 15.9219 - mae: 15.9219\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 17.0024 - mae: 17.0024\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 15.9835 - mae: 15.9835\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 15.6599 - mae: 15.6599\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 14.1629 - mae: 14.1629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1edd8ee3130>"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Modifying base model\n",
    "\n",
    "model_2 =  tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(100,activation='relu'),\n",
    "        tf.keras.layers.Dense(10,activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "model_2.compile(loss='mae',\n",
    "               optimizer=tf.keras.optimizers.Adam(learning_rate=.01),\n",
    "               metrics='mae')\n",
    "\n",
    "model_2.fit(X_train,y_train,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "0b76b406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 212ms/step - loss: 47.4931 - mae: 47.4931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[47.49310302734375, 47.49310302734375]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88d65608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13     -0.28\n",
       "39     -6.58\n",
       "30     22.71\n",
       "45     13.88\n",
       "17     36.22\n",
       "48    110.00\n",
       "26     75.89\n",
       "25    107.64\n",
       "32    -10.80\n",
       "19      2.52\n",
       "Name: Listing Day gain, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "087ad138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retail Investors</th>\n",
       "      <th>Qualified Investors</th>\n",
       "      <th>Non Instititional</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.35</td>\n",
       "      <td>32.41</td>\n",
       "      <td>33.91</td>\n",
       "      <td>17.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10.77</td>\n",
       "      <td>65.74</td>\n",
       "      <td>97.42</td>\n",
       "      <td>44.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>11.64</td>\n",
       "      <td>153.45</td>\n",
       "      <td>339.98</td>\n",
       "      <td>120.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>26.04</td>\n",
       "      <td>8.02</td>\n",
       "      <td>32.72</td>\n",
       "      <td>18.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>39.52</td>\n",
       "      <td>95.27</td>\n",
       "      <td>213.06</td>\n",
       "      <td>116.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>15.93</td>\n",
       "      <td>189.57</td>\n",
       "      <td>263.05</td>\n",
       "      <td>117.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.20</td>\n",
       "      <td>159.93</td>\n",
       "      <td>211.12</td>\n",
       "      <td>95.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12.57</td>\n",
       "      <td>168.58</td>\n",
       "      <td>238.04</td>\n",
       "      <td>102.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.09</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>42.04</td>\n",
       "      <td>49.83</td>\n",
       "      <td>116.30</td>\n",
       "      <td>64.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Retail Investors  Qualified Investors  Non Instititional   Total\n",
       "13              1.35                32.41              33.91   17.20\n",
       "39             10.77                65.74              97.42   44.06\n",
       "30             11.64               153.45             339.98  120.93\n",
       "45             26.04                 8.02              32.72   18.03\n",
       "17             39.52                95.27             213.06  116.71\n",
       "48             15.93               189.57             263.05  117.02\n",
       "26              9.20               159.93             211.12   95.54\n",
       "25             12.57               168.58             238.04  102.58\n",
       "32              3.09                 2.18               1.31    2.37\n",
       "19             42.04                49.83             116.30   64.40"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "739691d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 8.339438 ],\n",
       "        [26.465286 ],\n",
       "        [58.727333 ],\n",
       "        [ 9.744555 ],\n",
       "        [66.83241  ],\n",
       "        [80.36333  ],\n",
       "        [65.26459  ],\n",
       "        [68.83478  ],\n",
       "        [-2.9942653],\n",
       "        [40.267815 ]], dtype=float32),\n",
       " 13     -0.28\n",
       " 39     -6.58\n",
       " 30     22.71\n",
       " 45     13.88\n",
       " 17     36.22\n",
       " 48    110.00\n",
       " 26     75.89\n",
       " 25    107.64\n",
       " 32    -10.80\n",
       " 19      2.52\n",
       " Name: Listing Day gain, dtype: float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting on test data\n",
    "# model_2.predict(tf.expand_dims(X_test,axis=-1))\n",
    "y_preds_2=model_2.predict(X_test)\n",
    "y_preds_2,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fa2e1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_3 normalized model\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "ct = make_column_transformer(\n",
    "(MinMaxScaler(),['Retail Investors','Qualified Investors','Non Instititional','Total'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e2589021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('minmaxscaler', MinMaxScaler(),\n",
       "                                 ['Retail Investors', 'Qualified Investors',\n",
       "                                  'Non Instititional', 'Total'])])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3aadba9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('minmaxscaler', MinMaxScaler(),\n",
       "                                 ['Retail Investors', 'Qualified Investors',\n",
       "                                  'Non Instititional', 'Total'])])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.fit(X=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e6eea6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00571429, 0.16689127, 0.03460741, 0.04698206],\n",
       "       [0.06237594, 0.34655814, 0.09982748, 0.12664986],\n",
       "       [0.06760902, 0.81936284, 0.34891865, 0.35464927],\n",
       "       [0.15422556, 0.03541588, 0.03338536, 0.04944387],\n",
       "       [0.23530827, 0.50574093, 0.2185812 , 0.34213258],\n",
       "       [0.09341353, 1.01406932, 0.26991723, 0.34305205],\n",
       "       [0.05293233, 0.85429357, 0.21658896, 0.27934154],\n",
       "       [0.07320301, 0.90092178, 0.24423381, 0.30022245],\n",
       "       [0.01618045, 0.0039351 , 0.00112962, 0.0029957 ],\n",
       "       [0.25046617, 0.26079457, 0.11921584, 0.18697909]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_normal = ct.transform(X_train)\n",
    "X_test_normal = ct.transform(X_test)\n",
    "X_test_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "816cc4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1edd42f9bb0>"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_3 is normalized data model\n",
    "model_3 =  tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(100,activation='relu'),\n",
    "        tf.keras.layers.Dense(10,activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "model_3.compile(loss='mae',\n",
    "               optimizer=tf.keras.optimizers.Adam(learning_rate=.1),\n",
    "               metrics='mae')\n",
    "\n",
    "model_3.fit(X_train_normal,y_train,epochs=100,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "a3820b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 172ms/step - loss: 21.9454 - mae: 21.9454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[21.94536018371582, 21.94536018371582]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.evaluate(X_test_normal,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "060e7a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_3 = model_3.predict(X_test_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "e365865a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13     -0.28\n",
       "39     -6.58\n",
       "30     22.71\n",
       "45     13.88\n",
       "17     36.22\n",
       "48    110.00\n",
       "26     75.89\n",
       "25    107.64\n",
       "32    -10.80\n",
       "19      2.52\n",
       "Name: Listing Day gain, dtype: float64"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "ad40fe36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00571429, 0.16689127, 0.03460741, 0.04698206],\n",
       "       [0.06237594, 0.34655814, 0.09982748, 0.12664986],\n",
       "       [0.06760902, 0.81936284, 0.34891865, 0.35464927],\n",
       "       [0.15422556, 0.03541588, 0.03338536, 0.04944387],\n",
       "       [0.23530827, 0.50574093, 0.2185812 , 0.34213258],\n",
       "       [0.09341353, 1.01406932, 0.26991723, 0.34305205],\n",
       "       [0.05293233, 0.85429357, 0.21658896, 0.27934154],\n",
       "       [0.07320301, 0.90092178, 0.24423381, 0.30022245],\n",
       "       [0.01618045, 0.0039351 , 0.00112962, 0.0029957 ],\n",
       "       [0.25046617, 0.26079457, 0.11921584, 0.18697909]])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "59b07536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_predict(ipo,model):\n",
    "    \"\"\"takes input as list and returns predicted number\"\"\"\n",
    "    ipo = np.array(ipo)\n",
    "    ipo = tf.expand_dims(ipo,axis=0)\n",
    "    return model.predict(ipo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "c4654578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32.68999]], dtype=float32)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_predict(model=model_1,ipo=[57.91,32.91,166.69,73.94])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22f175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
