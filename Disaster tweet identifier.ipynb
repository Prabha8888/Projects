{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3429aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8f2e5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1bc7773f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Disaster tweet/train.csv\")\n",
    "test_df = pd.read_csv(\"Disaster tweet/test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378504db",
   "metadata": {},
   "source": [
    "import random\n",
    "random_int = random.randint(0,len(df))\n",
    "random_int\n",
    "\n",
    "for i in range(5):\n",
    "    random_int = random.randint(0,len(train_df))\n",
    "    print(f\"Target: {train_df['target'][random_int]}\" , \"(real disaster)\" if train_df['target'][random_int]>0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{df['text'][random_int]}\")\n",
    "    print('----\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "97193484",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_shuffled = train_df.sample(frac=1,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc6c21",
   "metadata": {},
   "source": [
    "Splitting our data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ffa00432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_sen,val_sen,train_labels,val_labels = train_test_split(train_df_shuffled['text'].to_numpy(),\n",
    "                                                             train_df_shuffled['target'].to_numpy(),\n",
    "                                                             random_state=42,\n",
    "                                                            test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b3b5d456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 762)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sen),len(val_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e9dc1997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "       'Imagine getting flattened by Kurt Zouma',\n",
       "       '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "       \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "       'Somehow find you and I collide http://t.co/Ee8RpOahPk'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#viewing first 5 training samples\n",
    "train_sen[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22dcc5",
   "metadata": {},
   "source": [
    "#writing a function that will turn our text data into numbers. for more info: \n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b2182037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average words per tweet\n",
    "avg_word = round(sum([len(i.split()) for i in train_sen])/len(train_sen))\n",
    "avg_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9bda4449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.preprocessing.text_vectorization.TextVectorization at 0x1fbf911cc40>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "text_vectorizer = TextVectorization(max_tokens=1000,\n",
    "                                   output_mode='int',\n",
    "                                   output_sequence_length=avg_word)\n",
    "text_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8a58764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer.adapt(train_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "626db716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[  8, 160, 104,   5,   1,  19,   1,   5,   1,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#before converting any text from train_sen lets convert any text to numbers using text_vectorizer layer\n",
    "text_to_number = text_vectorizer([\"I am going to convert this sentence to numbers\"])\n",
    "text_to_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e406d652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "\n",
      " Pretty teen Hayden Ryan poses and strips out of her purple top http://t.co/qew4c5M1xd View and download video\n",
      "----\n",
      "Vectorized text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[766,   1,   1,   1,   1,   7,   1,  36,   6,  81,   1, 212,   1,\n",
       "        976,   7]], dtype=int64)>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_text = random.choice(train_sen)\n",
    "print(f\"Original text:\\n\\n {random_text}\")\n",
    "print(\"----\")\n",
    "print(f\"Vectorized text:\")\n",
    "text_vectorizer([random_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "4c3e56c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 1000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 least common words: ['reported', 'r', 'pray', 'playlist', 'patience']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
    "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\") \n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56651b",
   "metadata": {},
   "source": [
    "Lets write a function to convert these numbers to feature vectors. For more info: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e9ca9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "57422356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x1fbff31bf40>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embedding(input_dim=1000,\n",
    "                           output_dim=128,\n",
    "                           input_length=avg_word,\n",
    "                           name='embedding_1')\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c2ec8c",
   "metadata": {},
   "source": [
    "Converting our 'text_to_number' variable to feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "fcf49e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[ 0.00643777,  0.00809912, -0.0333337 , ..., -0.0014866 ,\n",
       "         -0.02233075, -0.00463134],\n",
       "        [ 0.04601005,  0.02017364, -0.01703423, ...,  0.04098071,\n",
       "          0.02005376, -0.00837458],\n",
       "        [-0.00627888,  0.03613004, -0.03284682, ...,  0.04677888,\n",
       "         -0.0294435 , -0.04086154],\n",
       "        ...,\n",
       "        [-0.01204009,  0.01908055, -0.0382763 , ..., -0.03221116,\n",
       "         -0.02657125,  0.02125778],\n",
       "        [-0.01204009,  0.01908055, -0.0382763 , ..., -0.03221116,\n",
       "         -0.02657125,  0.02125778],\n",
       "        [-0.01204009,  0.01908055, -0.0382763 , ..., -0.03221116,\n",
       "         -0.02657125,  0.02125778]]], dtype=float32)>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_text = embedding_layer(text_to_number)\n",
    "embedded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38253c9c",
   "metadata": {},
   "source": [
    "We will now train the model using a pre trained model from tensorflow hub and fit that model to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "7bc3ba47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_hub.keras_layer.KerasLayer at 0x1fbff31b610>"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\",\n",
    "                                 trainable=False,\n",
    "                                 input_shape=[],\n",
    "                                 dtype='string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "06c4e8c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: C:\\Users\\prabh\\AppData\\Local\\Temp\\tfhub_modules\\c9fe785512ca4a1b179831acb18a0c6bfba603dd\\{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-591-2d471dc5c03b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m pre_trained_2 = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\",\n\u001b[0m\u001b[0;32m      2\u001b[0m                                  \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                  \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                  dtype='string')\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_is_hub_module_v1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(handle, tags, load_options)\u001b[0m\n\u001b[0;32m    447\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Expected before TF2.4.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_hub\\module_v2.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m    104\u001b[0m         module_path, tags=tags, options=options)\n\u001b[0;32m    105\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m   \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(export_dir, tags, options)\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   \"\"\"\n\u001b[1;32m--> 869\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"root\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[0;32m    879\u001b[0m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m   saved_model_proto, debug_info = (\n\u001b[1;32m--> 881\u001b[1;33m       loader_impl.parse_saved_model_with_debug_info(export_dir))\n\u001b[0m\u001b[0;32m    882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m   if (len(saved_model_proto.meta_graphs) == 1 and\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model_with_debug_info\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mparsed\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mMissing\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0minfo\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mfine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m   \"\"\"\n\u001b[1;32m---> 56\u001b[1;33m   \u001b[0msaved_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m   debug_info_path = os.path.join(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    111\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot parse file %s: %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m     raise IOError(\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;34m\"SavedModel file does not exist at: %s%s{%s|%s}\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: C:\\Users\\prabh\\AppData\\Local\\Temp\\tfhub_modules\\c9fe785512ca4a1b179831acb18a0c6bfba603dd\\{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "pre_trained_2 = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\",\n",
    "                                 trainable=False,\n",
    "                                 input_shape=[],\n",
    "                                 dtype='string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "eeb379b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_hub.keras_layer.KerasLayer at 0x1fbff31b610>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "fc1537b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = tf.keras.Sequential([\n",
    "    pretrained_model,\n",
    "#     text_vectorizer,\n",
    "#     embedding_layer,\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "    \n",
    "])\n",
    "\n",
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d0d1fa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 234s 534ms/step - loss: 0.5035 - accuracy: 0.7885 - val_loss: 0.4268 - val_accuracy: 0.8215\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 101s 472ms/step - loss: 0.4003 - accuracy: 0.8269 - val_loss: 0.4176 - val_accuracy: 0.8228\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 90s 420ms/step - loss: 0.3830 - accuracy: 0.8326 - val_loss: 0.4168 - val_accuracy: 0.8255\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 90s 418ms/step - loss: 0.3723 - accuracy: 0.8375 - val_loss: 0.4140 - val_accuracy: 0.8228\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 89s 415ms/step - loss: 0.3641 - accuracy: 0.8427 - val_loss: 0.4141 - val_accuracy: 0.8241\n"
     ]
    }
   ],
   "source": [
    "model_1_history = model_1.fit(train_sen,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sen, val_labels),\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "28c6df8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "0aa32b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.8\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd609d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5cfbdb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_1_disaster_tweet_local\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_1_disaster_tweet_local\\assets\n"
     ]
    }
   ],
   "source": [
    "model_1.save(\"model_1_disaster_tweet_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "4358b14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x1fa35ab2310>"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_1 = tf.keras.models.load_model('model_1_disaster_tweet_local')\n",
    "loaded_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9bce6d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 10s 395ms/step - loss: 0.4141 - accuracy: 0.8241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.41410452127456665, 0.8241469860076904]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(val_sen,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ab1f6ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 25s 403ms/step - loss: 0.4141 - accuracy: 0.8241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.41410452127456665, 0.8241469860076904]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_1.evaluate(val_sen,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b3605032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24357572],\n",
       "       [0.7291944 ],\n",
       "       [0.9931166 ],\n",
       "       [0.12791267],\n",
       "       [0.71145445],\n",
       "       [0.71302086],\n",
       "       [0.9891956 ],\n",
       "       [0.9541272 ],\n",
       "       [0.97506726],\n",
       "       [0.1465087 ]], dtype=float32)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob = loaded_model_1.predict(val_sen)\n",
    "pred_prob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "dc78a81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = tf.squeeze(tf.round(pred_prob)).numpy()\n",
    "pred_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "88be1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_sen(model,sentence):\n",
    "    pred_prob = model.predict([sentence])\n",
    "    pred_label = tf.squeeze(tf.round(pred_prob)).numpy()\n",
    "    print(f\"Pred: {pred_label}\", \"(real disaster)\" if pred_label > 0 else \"(not real disaster)\",f\"Prob: {pred_prob[0][0]}\")\n",
    "    print(f\"Text:\\n{sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "a4a689c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 1.0 (real disaster) Prob: 0.5109326839447021\n",
      "Text:\n",
      "flood and Dead bodies everywhere..Good God bless these souls\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = 'flood and Dead bodies everywhere..Good God bless these souls'\n",
    "predict_on_sen(model=loaded_model_1,sentence=sentence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "36418230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Target</th>\n",
       "      <th>Pred_label</th>\n",
       "      <th>Pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DFR EP016 Monthly Meltdown - On Dnbheaven 2015...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.243576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FedEx no longer to transport bioterror germs i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.729194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gunmen kill four in El Salvador bus attack: Su...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@camilacabello97 Internally and externally scr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.127913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radiation emergency #preparedness starts with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.711454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Investigators rule catastrophic structural fai...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.713021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How the West was burned: Thousands of wildfire...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.989196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Map: Typhoon Soudelor's predicted path as it a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.954127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ûª93 blasts accused Yeda Yakub dies in Karach...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>My ears are bleeding  https://t.co/k5KnNwugwT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Target  Pred_label  \\\n",
       "0  DFR EP016 Monthly Meltdown - On Dnbheaven 2015...       0         0.0   \n",
       "1  FedEx no longer to transport bioterror germs i...       0         1.0   \n",
       "2  Gunmen kill four in El Salvador bus attack: Su...       1         1.0   \n",
       "3  @camilacabello97 Internally and externally scr...       1         0.0   \n",
       "4  Radiation emergency #preparedness starts with ...       1         1.0   \n",
       "5  Investigators rule catastrophic structural fai...       1         1.0   \n",
       "6  How the West was burned: Thousands of wildfire...       1         1.0   \n",
       "7  Map: Typhoon Soudelor's predicted path as it a...       1         1.0   \n",
       "8  Ûª93 blasts accused Yeda Yakub dies in Karach...       1         1.0   \n",
       "9      My ears are bleeding  https://t.co/k5KnNwugwT       0         0.0   \n",
       "\n",
       "   Pred_prob  \n",
       "0   0.243576  \n",
       "1   0.729194  \n",
       "2   0.993117  \n",
       "3   0.127913  \n",
       "4   0.711454  \n",
       "5   0.713021  \n",
       "6   0.989196  \n",
       "7   0.954127  \n",
       "8   0.975067  \n",
       "9   0.146509  "
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_pred_df = pd.DataFrame({\n",
    "    'Text': val_sen,\n",
    "    'Target':val_labels,\n",
    "    'Pred_label':pred_labels,\n",
    "    'Pred_prob':tf.squeeze(pred_prob)\n",
    "})\n",
    "\n",
    "wrong_pred_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "4a37be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_wrong = wrong_pred_df[wrong_pred_df['Target']!=wrong_pred_df['Pred_label']].sort_values(by='Pred_prob',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "671c9816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Target</th>\n",
       "      <th>Pred_label</th>\n",
       "      <th>Pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>Emergency Response and Hazardous Chemical Mana...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.918610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>? High Skies - Burning Buildings ? http://t.co...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.917468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>@madonnamking RSPCA site multiple 7 story high...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.877314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Five Fatal Flaws in the Iran Deal https://...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.869626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>Trafford Centre film fans angry after Odeon ci...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.864984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>Air Group is here to the rescue! We have 24/7 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.822100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>FedEx will no longer transport bioterror patho...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.789781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>Haley Lu Richardson Fights for Water in The La...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.788961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Photo: postapocalypticflimflam: Prodding aroun...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.784829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>@noah_anyname That's where the concentration c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.770379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Target  Pred_label  \\\n",
       "619  Emergency Response and Hazardous Chemical Mana...       0         1.0   \n",
       "31   ? High Skies - Burning Buildings ? http://t.co...       0         1.0   \n",
       "49   @madonnamking RSPCA site multiple 7 story high...       0         1.0   \n",
       "11   The Five Fatal Flaws in the Iran Deal https://...       0         1.0   \n",
       "303  Trafford Centre film fans angry after Odeon ci...       0         1.0   \n",
       "344  Air Group is here to the rescue! We have 24/7 ...       0         1.0   \n",
       "759  FedEx will no longer transport bioterror patho...       0         1.0   \n",
       "718  Haley Lu Richardson Fights for Water in The La...       0         1.0   \n",
       "317  Photo: postapocalypticflimflam: Prodding aroun...       0         1.0   \n",
       "628  @noah_anyname That's where the concentration c...       0         1.0   \n",
       "\n",
       "     Pred_prob  \n",
       "619   0.918610  \n",
       "31    0.917468  \n",
       "49    0.877314  \n",
       "11    0.869626  \n",
       "303   0.864984  \n",
       "344   0.822100  \n",
       "759   0.789781  \n",
       "718   0.788961  \n",
       "317   0.784829  \n",
       "628   0.770379  "
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_wrong[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "46aae82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df) == len(train_sen)+len(val_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "672f4437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emergency Response and Hazardous Chemical Management: Principles and Practices http://t.co/4sSuyhkgRB http://t.co/TDerBtgZ2k\n",
      "1527    2209.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "? High Skies - Burning Buildings ? http://t.co/uVq41i3Kx2 #nowplaying\n",
      "1206    1737.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "@madonnamking RSPCA site multiple 7 story high rise buildings next to low density character residential in an area that floods\n",
      "3991    5670.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "The Five Fatal Flaws in the Iran Deal https://t.co/ztfEAd8GId via @YouTube\n",
      "3594    5134.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "Trafford Centre film fans angry after Odeon cinema evacuated following false fire alarm   http://t.co/6GLDwx71DA\n",
      "3343    4787.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "Air Group is here to the rescue! We have 24/7 Emergency Service! Learn more about it here - http://t.co/9lyx7zMtHE http://t.co/5PbC96rTMJ\n",
      "3226    4630.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "FedEx will no longer transport bioterror pathogens in wake of anthrax lab mishaps http://t.co/lHpgxc4b8J\n",
      "578    836.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "Haley Lu Richardson Fights for Water in The Last Survivors (Review) http://t.co/oObSCFOKtQ\n",
      "6592    9439.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "Photo: postapocalypticflimflam: Prodding around the rubble. http://t.co/Bgy4i47j70\n",
      "5836    8339.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "@noah_anyname That's where the concentration camps and mass murder come in. \n",
      " \n",
      "EVERY. FUCKING. TIME.\n",
      "4821    6862.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "Day 2. Liquidation of emergency at chemical object. #USAR2015 #USAR15 #RUOR #??????????? http://t.co/gGTmDqUdDo\n",
      "1552    2241.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "Ashes 2015: AustraliaÛªs collapse at Trent Bridge among worst in history: England bundled out Australia for 60 ... http://t.co/t5TrhjUAU0\n",
      "1186    1707.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "GENERAL AUDIENCE: On Wounded Families | ZENIT - The World Seen From Rome http://t.co/hFvnyfT78C\n",
      "7414    10607.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/qZQc8WWwcN via @usatoday\n",
      "584    843.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "@Azimel 'Screaming Mad Scientist deceased after tumbling over heels and falling into sinkhole during investigation'\n",
      "6054    8653.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "#Islamic #state issue a new holiday #brochure lovely swimming pool for drowning in shooting range and the downside it costs a #bomb\n",
      "2982    4282.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "@SonofLiberty357 all illuminated by the brightly burning buildings all around the town!\n",
      "1195    1721.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "the windstorm blew thru my open window and now my bong is in pieces just another example of nature's indifference to human suffering\n",
      "7393    10580.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "My phone looks like it was in a car ship airplane accident. Terrible\n",
      "166    241.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "@LegacyOfTheSith @SagaciousSaber @Lordofbetrayal Moved in a crescent formation small trails of dust left in their wake as they moved.\n",
      "3019    4334.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#finding the most wrong predicted samples in training dataset\n",
    "# false positive\n",
    "for row in most_wrong[:20].itertuples():\n",
    "    _,text,_,_,_ = row\n",
    "    print(text)\n",
    "    x = train_df['id'][train_df['text']==text]\n",
    "    print(x,'\\n','----','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "189ccdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@willienelson We need help! Horses will die!Please RT &amp; sign petition!Take a stand &amp; be a voice for them! #gilbert23 https://t.co/e8dl1lNCVu\n",
      "257    365.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "New post from @darkreading http://t.co/8eIJDXApnp New SMB Relay Attack Steals User Credentials Over Internet\n",
      "461    665.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "If I fall is men GOD @Praiz8 is d bomb well av always known dat since 2008 bigger u I pray sir\n",
      "1080    1560.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "I get to smoke my shit in peace\n",
      "6221    8880.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "@reriellechan HE WAS THE LICH KING'S FIRST CASUALTY BLOCK ME BACK I HATE YOU! http://t.co/0Gidg9U45J\n",
      "1454    2097.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "Sadly before she could save humanity Ursula drowned in the drool of a protoshoggoth but at least she sort of died doing what she loved.\n",
      "2923    4202.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "Just came back from camping and returned with a new song which gets recorded tomorrow. Can't wait! #Desolation #TheConspiracyTheory #NewEP\n",
      "2511    3607.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "You can never escape me. Bullets don't harm me. Nothing harms me. But I know pain. I know pain. Sometimes I share it. With someone like you.\n",
      "4154    5903.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "@Habbo bring back games from the past. Snowstorm. Tic tac toe. Battleships. Fast food. Matchwood.\n",
      "6239    8908.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "@DavidVonderhaar At least you were sincere ??\n",
      "5596    7984.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "'The way you move is like a full on rainstorm and I'm a house of cards'\n",
      "5574    7954.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "I Will Survive by Gloria Gaynor (with Oktaviana Devi) ÛÓ https://t.co/HUkJZ1wT36\n",
      "6514    9315.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "VICTORINOX SWISS ARMY DATE WOMEN'S RUBBER MOP WATCH 241487 http://t.co/yFy3nkkcoH http://t.co/KNEhVvOHVK\n",
      "361    519.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "Ron &amp; Fez - Dave's High School Crush https://t.co/aN3W16c8F6 via @YouTube\n",
      "1861    2675.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "@SoonerMagic_ I mean I'm a fan but I don't need a girl sounding off like a damn siren\n",
      "6160    8786.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "going to redo my nails and watch behind the scenes of desolation of smaug ayyy\n",
      "2522    3625.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "Leitchfield KY:\n",
      "\n",
      " Bella Edward &amp; Rosalie need rescue/adoption/local foster home(s)/sponsorships.\n",
      "\n",
      " Trapped &amp;... http://t.co/Ajay0sNPlg\n",
      "6812    9759.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "Next May I'll be free...from school from obligations like family.... Best of all that damn curfew...\n",
      "1934    2782.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "Why are you deluged with low self-image? Take the quiz: http://t.co/XsPqdOrIqj http://t.co/CQYvFR4UCy\n",
      "2250    3221.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n",
      "#download &amp; #watch Demolition Frog (2002) http://t.co/81nEizeknm #movie\n",
      "2342    3369.0\n",
      "Name: id, dtype: float64 \n",
      " ---- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# false negative\n",
    "for row in most_wrong[-20:].itertuples():\n",
    "    _,text,_,_,_ = row\n",
    "    print(text)\n",
    "    x = train_df['id'][train_df['text']==text]\n",
    "    print(x,'\\n','----','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "a0a5b37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2209.0, 1737.0, 5670.0, 5134.0, 4787.0, 4630.0, 836.0, 9439.0, 8339.0, 6862.0]\n"
     ]
    }
   ],
   "source": [
    "top_10_false_positive_id=[]\n",
    "for row in most_wrong[:10].itertuples():\n",
    "    _,text,_,_,_ = row\n",
    "#     print(text)\n",
    "    x = train_df['id'][train_df['text']==text]\n",
    "#     print(x)\n",
    "    for i in x:\n",
    "        top_10_false_positive_id.append(i)\n",
    "print(top_10_false_positive_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "49b6c92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7954.0, 9315.0, 519.0, 2675.0, 8786.0, 3625.0, 9759.0, 2782.0, 3221.0, 3369.0]\n"
     ]
    }
   ],
   "source": [
    "top_10_false_negative_id=[]\n",
    "for row in most_wrong[-10:].itertuples():\n",
    "    _,text,_,_,_ = row\n",
    "#     print(text)\n",
    "    x = train_df['id'][train_df['text']==text]\n",
    "#     print(x)\n",
    "    for i in x:\n",
    "        top_10_false_negative_id.append(i)\n",
    "print(top_10_false_negative_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c1ec4",
   "metadata": {},
   "source": [
    "Lets manually correct the dataset and reimport it and re train it on same train and test data and see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "11963fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1527, 1206, 3991, 3594, 3343, 3226, 578, 6592, 5836, 4821]\n"
     ]
    }
   ],
   "source": [
    "top_10_false_positive_index=[]\n",
    "for row in most_wrong[:10].itertuples():\n",
    "    _,text,_,_,_ = row\n",
    "    x = train_df[train_df['text']==text]\n",
    "    for i in x.itertuples():\n",
    "        index,_,_,_,_,_ = i\n",
    "        top_10_false_positive_index.append(index)\n",
    "print(top_10_false_positive_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "29451e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[257, 461, 1080, 6221, 1454, 2923, 2511, 4154, 6239, 5596, 5574, 6514, 361, 1861, 6160, 2522, 6812, 1934, 2250, 2342]\n"
     ]
    }
   ],
   "source": [
    "top_20_false_negative_index=[]\n",
    "for row in most_wrong[-20:].itertuples():\n",
    "    _,text,_,_,_ = row\n",
    "    x = train_df[train_df['text']==text]\n",
    "    for i in x.itertuples():\n",
    "        index,_,_,_,_,_ = i\n",
    "        top_20_false_negative_index.append(index)\n",
    "print(top_20_false_negative_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "ca9c30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_df = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "d1f3fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in top_20_false_negative_index:\n",
    "    corrected_df.at[i,'target']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "f0fb078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_df.to_csv('C:\\\\Users\\\\prabh\\\\Videos\\\\Disaster tweet\\\\corrected disaster tweet train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "2cb2fecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                      365.0\n",
       "keyword                                          annihilation\n",
       "location                                                  NaN\n",
       "text        @willienelson We need help! Horses will die!Pl...\n",
       "target                                                    0.0\n",
       "Name: 257, dtype: object"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[257]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "073d8451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0                    Just happened a terrible car crash  \n",
       "1     Heard about #earthquake is different cities, s...  \n",
       "2     there is a forest fire at spot pond, geese are...  \n",
       "3              Apocalypse lighting. #Spokane #wildfires  \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                 ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "3259  Storm in RI worse than last hurricane. My city...  \n",
       "3260  Green Line derailment in Chicago http://t.co/U...  \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
       "3262  #CityofCalgary has activated its Municipal Eme...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "8acf7736",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=[]\n",
    "target=[]\n",
    "for row in test_df.itertuples():\n",
    "    _,id,_,_,text=row\n",
    "    ids.append(id)\n",
    "#     print(text)\n",
    "    pred_prob = loaded_model_1.predict([text])\n",
    "    pred_label = tf.squeeze(tf.round(pred_prob)).numpy()\n",
    "    target.append(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9421f069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "d5b33dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0     1.0\n",
       "1   2     1.0\n",
       "2   3     1.0\n",
       "3   9     1.0\n",
       "4  11     1.0"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'id':ids,\n",
    "    'target':target\n",
    "})\n",
    "\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "edf23a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 3263)"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submission_df),len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "cbd4b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_df[\"text\"].to_list()\n",
    "test_samples = random.sample(test_sentences, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "93e165ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT- Udhampur terror attack: Militants attack police post 2 SPOs injured: Suspected militants tonight at... http://t.co/wN414hp9hZ #News',\n",
       " \"I'm an emotional wreck someone hold me until they upload the damn video #whatstheimportantvideo\",\n",
       " \"@ReasonVsFear 'One?'  Oh nonononono.  This hijacking's been going on forever but I've seen it about a dozen diff places today alone.\",\n",
       " '@hitchBOT I heard you might rise from the ashes of Armageddon. .go hitchbot ..like the true superstar you are.',\n",
       " '11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...',\n",
       " 'Rape victim dies as she sets herself ablaze: A 16-year-old girl died of burn injuries as she set herself ablaze\\x89Û_ http://t.co/UK8hNrbOob',\n",
       " '#Business People are finally panicking about cable TV http://t.co/FcfSSlxMy2 http://t.co/9xFzkpdmQz',\n",
       " '@al_thegoon i like hurricane habor next to slix flags but they got like 8 rides but they wavy &amp; hershey park',\n",
       " 'business casualty',\n",
       " 'Pope Francis talks about WOUNDED FAMILIES\\nhttp://t.co/m1RfBPODI9']"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "aaccf41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "# https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate/34294022\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "fc985a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C\n",
      "t\n",
      "@bbcmtd Wholesale Markets ablaze \n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r\"https?://(\\S+|www)\\.\\S+\")\n",
    "for t in df.text:\n",
    "    matches = pattern.findall(t)\n",
    "    for match in matches:\n",
    "        print(t)\n",
    "        print(match)\n",
    "        print(pattern.sub(r\"\", t))\n",
    "    if len(matches) > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "8e83f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Disaster tweet/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "baadf160",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df.text.map(remove_URL) # map(lambda x: remove_URL(x))\n",
    "df[\"text\"] = df.text.map(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "5dc9bbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prabh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "# pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine\n",
    "# has been programmed to ignore, both when indexing entries for searching and when retrieving them \n",
    "# as the result of a search query.\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "# https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "2d65f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df.text.map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "8ceea094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import train_test_split\n",
    "\n",
    "train_sen_filter,val_sen_filter,train_labels,val_labels = train_test_split(df['text'],\n",
    "                                                                          df['target'],\n",
    "                                                                          random_state=42,\n",
    "                                                                          test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "d4ccc93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 762)"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sen_filter),len(val_sen_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "f8d56f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tf.keras.Sequential([\n",
    "    pretrained_model,\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "    \n",
    "])\n",
    "\n",
    "model_2.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "c6301c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 163s 639ms/step - loss: 0.5065 - accuracy: 0.7889 - val_loss: 0.4178 - val_accuracy: 0.8189\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 112s 520ms/step - loss: 0.4139 - accuracy: 0.8219 - val_loss: 0.3998 - val_accuracy: 0.8268\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 99s 461ms/step - loss: 0.3981 - accuracy: 0.8251 - val_loss: 0.3965 - val_accuracy: 0.8320\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 100s 466ms/step - loss: 0.3871 - accuracy: 0.8335 - val_loss: 0.3989 - val_accuracy: 0.8255\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 99s 461ms/step - loss: 0.3796 - accuracy: 0.8348 - val_loss: 0.3965 - val_accuracy: 0.8255\n"
     ]
    }
   ],
   "source": [
    "model_2_history = model_2.fit(train_sen_filter,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sen_filter, val_labels),\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "45cc871e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 6s 265ms/step - loss: 0.3965 - accuracy: 0.8255\n",
      "24/24 [==============================] - 7s 266ms/step - loss: 0.3704 - accuracy: 0.8346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.39649683237075806, 0.8254593014717102],\n",
       " [0.37043341994285583, 0.834645688533783])"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.evaluate(val_sen_filter,val_labels),model_1.evaluate(val_sen_filter,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "c2c86c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"Is this a disaster tweet\"\n",
    "sentence_2 = 'flood and Dead bodies everywhere..Good God bless these souls'\n",
    "sentence_3 = \"Life is so fickle and unpredictable. I cannot process the passing of this great of our sport and also a person I got to know off the field. RIP #goat. Greatest to turn the cricket ball.\"\n",
    "sentence_4 = \"NSW floods live updates: Evacuation warning as Manly Dam spills, Sydney roads closed for dangerous flash flooding - ABC News \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "22aacda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 0.0 (not real disaster) Prob: 0.24924486875534058\n",
      "Text:\n",
      "Is this a disaster tweet\n",
      "\n",
      "\n",
      "Pred: 1.0 (real disaster) Prob: 0.5465414524078369\n",
      "Text:\n",
      "flood and Dead bodies everywhere..Good God bless these souls\n",
      "\n",
      "\n",
      "Pred: 0.0 (not real disaster) Prob: 0.248977392911911\n",
      "Text:\n",
      "Life is so fickle and unpredictable. I cannot process the passing of this great of our sport and also a person I got to know off the field. RIP #goat. Greatest to turn the cricket ball.\n",
      "\n",
      "\n",
      "Pred: 1.0 (real disaster) Prob: 0.9923743009567261\n",
      "Text:\n",
      "NSW floods live updates: Evacuation warning as Manly Dam spills, Sydney roads closed for dangerous flash flooding - ABC News \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [sentence_1,sentence_2,sentence_3,sentence_4]:\n",
    "  predict_on_sen(model_2,i)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "f82d485b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Op type not registered 'SentencepieceOp' in binary running on PRABHANSHU. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n If trying to load on a different device from the computational device, consider using setting the `experimental_io_device` option on tf.saved_model.LoadOptions to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_op_def\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m   3957\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3958\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_op_def_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3959\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SentencepieceOp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[0;32m    904\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 905\u001b[1;33m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0m\u001b[0;32m    906\u001b[0m                             ckpt_options, filters)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, filters)\u001b[0m\n\u001b[0;32m    132\u001b[0m     self._concrete_functions = (\n\u001b[1;32m--> 133\u001b[1;33m         function_deserialization.load_function_def_library(\n\u001b[0m\u001b[0;32m    134\u001b[0m             meta_graph.graph_def.library))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\u001b[0m in \u001b[0;36mload_function_def_library\u001b[1;34m(library, load_shared_name_suffix)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m       \u001b[0mfunc_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction_def_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_def_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m     \u001b[0m_restore_gradient_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenamed_functions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function_def_to_graph.py\u001b[0m in \u001b[0;36mfunction_def_to_graph\u001b[1;34m(fdef, input_shapes)\u001b[0m\n\u001b[0;32m     62\u001b[0m       \u001b[0minput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shapes_attr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m   graph_def, nested_to_flat_tensor_name = function_def_to_graph_def(\n\u001b[0m\u001b[0;32m     64\u001b[0m       fdef, input_shapes)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function_def_to_graph.py\u001b[0m in \u001b[0;36mfunction_def_to_graph_def\u001b[1;34m(fdef, input_shapes)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m       \u001b[0mop_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_op_def\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m   3961\u001b[0m         \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3962\u001b[1;33m         pywrap_tf_session.TF_GraphGetOpDef(self._c_graph, compat.as_bytes(type),\n\u001b[0m\u001b[0;32m   3963\u001b[0m                                            buf)\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Op type not registered 'SentencepieceOp' in binary running on PRABHANSHU. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-586-a2e8a628c9d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m pre_trained_2 = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\",\n\u001b[0m\u001b[0;32m      2\u001b[0m                                  \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                  \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                  dtype='string')\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_is_hub_module_v1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(handle, tags, load_options)\u001b[0m\n\u001b[0;32m    447\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Expected before TF2.4.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_hub\\module_v2.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m    104\u001b[0m         module_path, tags=tags, options=options)\n\u001b[0;32m    105\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m   \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(export_dir, tags, options)\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   \"\"\"\n\u001b[1;32m--> 869\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"root\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[0;32m    906\u001b[0m                             ckpt_options, filters)\n\u001b[0;32m    907\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m         raise FileNotFoundError(\n\u001b[0m\u001b[0;32m    909\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n If trying to load on a different device from the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;34m\"computational device, consider using setting the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Op type not registered 'SentencepieceOp' in binary running on PRABHANSHU. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n If trying to load on a different device from the computational device, consider using setting the `experimental_io_device` option on tf.saved_model.LoadOptions to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "pre_trained_2 = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\",\n",
    "                                 trainable=False,\n",
    "                                 input_shape=[],\n",
    "                                 dtype='string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998d0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = tf.keras.Sequential([\n",
    "    pre_trained_2,\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "    \n",
    "])\n",
    "\n",
    "model_3.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175627d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_history = model_3.fit(train_sen_filter,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sen_filter, val_labels),\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30247b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.evaluate(val_sen_filter,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb16848",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [sentence_1,sentence_2,sentence_3,sentence_4]:\n",
    "  predict_on_sen(model_3,i)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c75e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
